{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from os import makedirs, path, listdir, remove\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import zipfile as zpf\n",
    "from shutil import rmtree\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import squareform, pdist, cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import cm\n",
    "import re\n",
    "\n",
    "import httplib2\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93539ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load map file\n",
    "folder='tmp'\n",
    "london_boroughs_gdf = gpd.read_file(path.join(folder, \"london_boroughs_coordinates.shp\"))\n",
    "london_gdf = london_boroughs_gdf.dissolve()\n",
    "print(london_boroughs_gdf.shape)\n",
    "london_boroughs_gdf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN metadata\n",
    "london_sites_gdf = gpd.read_file(path.join(folder, \"LAQN_sites.shp\"))\n",
    "print(london_sites_gdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bdf72",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN data\n",
    "\n",
    "NO2_raw = pd.read_csv(\"tmp/LAQN_NO2_1996-01-01_2021-01-01.csv\")\n",
    "CO_raw = pd.read_csv(\"tmp/LAQN_CO_1996-01-01_2021-01-01.csv\")\n",
    "O3_raw = pd.read_csv(\"tmp/LAQN_O3_1996-01-01_2021-01-01.csv\")\n",
    "SO2_raw = pd.read_csv(\"tmp/LAQN_SO2_1996-01-01_2021-01-01.csv\")\n",
    "PM10_raw = pd.read_csv(\"tmp/LAQN_PM10_1996-01-01_2021-01-01.csv\")\n",
    "PM25_raw = pd.read_csv(\"tmp/LAQN_PM25_1996-01-01_2021-01-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413abff4",
   "metadata": {},
   "source": [
    "## Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM10_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db512602",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM10_raw.to_csv('incomplete_PM10_hourly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NO2_raw.shape)\n",
    "print(CO_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe(df, species, freq='D'):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df = df.groupby(pd.Grouper(key=\"date\", freq='D')).mean()\n",
    "    df = df.reset_index(level=0)\n",
    "    df['species'] = species\n",
    "    df = df[ ['date', 'species'] + [ col for col in df.columns if col not in ['date', 'species'] ] ]\n",
    "#     df = df.groupby(pd.Grouper(key=\"date\", freq='D')).mean()\n",
    "#     df = df.reset_index(level=0)\n",
    "    return df\n",
    "\n",
    "NO2_test = format_dataframe(NO2_raw, 'NO2')\n",
    "CO_test = format_dataframe(CO_raw, 'CO')\n",
    "O3_test = format_dataframe(O3_raw, 'O3')\n",
    "SO2_test = format_dataframe(SO2_raw, 'SO2')\n",
    "PM10_test = format_dataframe(PM10_raw, 'PM10')\n",
    "PM25_test = format_dataframe(PM25_raw, 'PM2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85771fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NO2_test.shape)\n",
    "print(CO_test.shape)\n",
    "print(O3_test.shape)\n",
    "print(SO2_test.shape)\n",
    "print(PM10_test.shape)\n",
    "print(PM25_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a736877",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "species = ['NO2', 'CO', 'O3', 'SO2', 'PM2.5', 'PM10']\n",
    "stations = [203, 45, 58, 55, 46, 176]\n",
    "plt.bar(species, stations, edgecolor='black')\n",
    "plt.title('Stations present in LAQN data per species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('No. of stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stations(df1, df2):\n",
    "    df1_stations = set(df1.columns.values[1:])\n",
    "    df2_stations = set(df2.columns.values[1:])\n",
    "    total = df1_stations.union(df2_stations)\n",
    "    print(f'df1 stations: {len(df1_stations)}')\n",
    "    print(f'df2 stations: {len(df2_stations)}')\n",
    "    print(f'total stations: {len(total)}')\n",
    "    \n",
    "compare_stations(NO2_raw, PM10_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = NO2_test.merge(CO_test, 'outer')\n",
    "full_dataset = full_dataset.merge(O3_test, 'outer')\n",
    "full_dataset = full_dataset.merge(SO2_test, 'outer')\n",
    "full_dataset = full_dataset.merge(PM10_test, 'outer')\n",
    "full_dataset = full_dataset.merge(PM25_test, 'outer')\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dataframe(df, freq='M', reset=False):\n",
    "    grouped_df = df.copy() \n",
    "    if reset:\n",
    "        grouped_df = grouped_df.reset_index(level=0)\n",
    "    grouped_df['date'] = pd.to_datetime(grouped_df.date)\n",
    "    grouped_df = grouped_df.groupby(pd.Grouper(key=\"date\", freq=freq)).mean()\n",
    "    return grouped_df\n",
    "\n",
    "def plot_station_data(df, station):\n",
    "    NO2   = group_dataframe(df[df['species'] == 'NO2'])\n",
    "    CO    = group_dataframe(df[df['species'] == 'CO'])\n",
    "    O3    = group_dataframe(df[df['species'] == 'O3'])\n",
    "    SO2   = group_dataframe(df[df['species'] == 'SO2'])\n",
    "    PM25  = group_dataframe(df[df['species'] == 'PM25'])\n",
    "    PM10  = group_dataframe(df[df['species'] == 'PM10'])\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'STATION - {station}', fontsize=20)\n",
    "    axs[0, 0].plot(NO2.index.values, NO2[station].values)\n",
    "    axs[0, 0].set_title('NO$_{2}$')\n",
    "    axs[0, 1].plot(CO.index.values, CO[station].values)\n",
    "    axs[0, 1].set_title('CO')\n",
    "    axs[1, 0].plot(O3.index.values, O3[station].values)\n",
    "    axs[1, 0].set_title('O$_{3}$')\n",
    "    axs[1, 1].plot(SO2.index.values, SO2[station].values)\n",
    "    axs[1, 1].set_title('SO$d_{2}$')\n",
    "    axs[2, 0].plot(PM25.index.values, PM25[station].values)\n",
    "    axs[2, 0].set_title('PM$_{2.5}$')\n",
    "    axs[2, 1].plot(PM10.index.values, PM10[station].values)\n",
    "    axs[2, 1].set_title('PM$_{10}$')\n",
    "#     fig.subplots_adjust(bottom = 0.5)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='date', ylabel='Concentration (µg/m$^3$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff44fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_list = full_dataset.columns[2:].to_numpy()\n",
    "sampled = np.random.choice(station_list, 10, replace=False)\n",
    "for station in sampled:\n",
    "    plot_station_data(full_dataset, station)\n",
    "    \n",
    "'''\n",
    "Stations with lot of species data: BX1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cb8f9",
   "metadata": {},
   "source": [
    "## Control Dataset Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9159472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_species(df, species, freq='D'):\n",
    "    data = df.copy()\n",
    "    data = data[data['species'] == species]\n",
    "    grouped_data = group_dataframe(data, freq)\n",
    "    return grouped_data\n",
    "\n",
    "# Test\n",
    "test_data = select_species(full_dataset, 'NO2')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1903010",
   "metadata": {},
   "source": [
    "## Histogram of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of missing data and stations\n",
    "\n",
    "def missing_data(df):\n",
    "    species_list = ['NO2', 'CO', 'O3', 'SO2', 'PM2.5', 'PM10']\n",
    "    for i, species in enumerate(species_list):\n",
    "        data = select_species(df, species)\n",
    "        print(f'Species: {species}')\n",
    "        print(f'Total: {data.size}')\n",
    "        print(f'Missing: {data.isna().sum().sum()}')\n",
    "        print(f'Proportion available: {100 * data.isna().sum().sum()/data.size}')\n",
    "\n",
    "def plot_missing_data(df):\n",
    "    species_list = ['NO2', 'CO', 'O3', 'SO2', 'PM2.5', 'PM10']\n",
    "    species_list2 = ['NO_2', 'CO', 'O_3', 'SO_2', 'PM_{2.5}', 'PM_{10}']\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 15))\n",
    "    fig.suptitle('Histogram of missing data', fontsize=24)\n",
    "    for i, species in enumerate(species_list):\n",
    "        row, col = i // 2, i % 2\n",
    "\n",
    "        data = select_species(df, species)\n",
    "        total = len(data)\n",
    "        num_missing = data.isna().sum()\n",
    "        \n",
    "        \n",
    "        s2 = species_list2[i]\n",
    "        axs[row, col].hist(num_missing / total, bins=10, edgecolor = \"black\")\n",
    "        axs[row, col].set_title(r'$ %s $' % species_list2[i], fontsize=20)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        for ax in axs.flat:\n",
    "#             ax.set(xlabel='Proportion of missing data', ylabel='No. of stations')\n",
    "            ax.set_xlabel('Proportion of missing data', fontsize=16)\n",
    "            ax.set_ylabel('No. of stations', fontsize=16)\n",
    "            ax.set_ylim([0, 200])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        \n",
    "#         plt.hist(num_missing / total, bins=10, edgecolor = \"black\")\n",
    "#         plt.title(species)\n",
    "#         plt.xlabel('Proportion of missing data')\n",
    "#         plt.ylabel('No. of stations')\n",
    "\n",
    "def plot_missing_data2(df):\n",
    "    species_list = ['NO2', 'CO', 'O3', 'SO2', 'PM2.5', 'PM10']\n",
    "    species_list2 = ['NO_2', 'CO', 'O_3', 'SO_2', 'PM_{2.5}', 'PM_{10}']\n",
    "    \n",
    "    fig, axs = plt.subplots(6, figsize=(20, 15))\n",
    "    fig.suptitle('Histogram of missing data', fontsize=24)\n",
    "    for i, species in enumerate(species_list):\n",
    "\n",
    "        data = select_species(df, species)\n",
    "        total = len(data)\n",
    "        num_missing = data.isna().sum()\n",
    "        \n",
    "        \n",
    "        s2 = species_list2[i]\n",
    "        axs[i].hist(num_missing / total, bins=10, edgecolor = \"black\")\n",
    "        axs[i].set_title(r'$ %s $' % species_list2[i], fontsize=20)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        for ax in axs.flat:\n",
    "#             ax.set(xlabel='Proportion of missing data', ylabel='No. of stations')\n",
    "            ax.set_xlabel('Proportion of missing data', fontsize=16)\n",
    "            ax.set_ylabel('No. of stations', fontsize=16)\n",
    "            ax.set_ylim([0, 200])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        \n",
    "#         plt.hist(num_missing / total, bins=10, edgecolor = \"black\")\n",
    "#         plt.title(species)\n",
    "#         plt.xlabel('Proportion of missing data')\n",
    "#         plt.ylabel('No. of stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = ['NO2', 'CO', 'O3', 'SO2', 'PM2.5', 'PM10']\n",
    "for species in species_list:\n",
    "    if species != 'CO':\n",
    "        m = re.search(r\"\\d\", species).start()\n",
    "    else:\n",
    "        m = len(species)\n",
    "    print(species[:m], species[m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fcc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_data(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df01759",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM10_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 5))\n",
    "data = select_species(full_dataset, 'NO2')\n",
    "# data = NO2_test\n",
    "total = len(data)\n",
    "num_missing = data.isna().sum()\n",
    "plt.hist((num_missing / total)*100, bins=10, edgecolor = \"black\")\n",
    "plt.title(r'$NO_2$', fontsize=20)\n",
    "plt.xlabel('Proportion of missing data (%)', fontsize=18)\n",
    "plt.ylabel('No. of stations', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a971d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 5))\n",
    "data = select_species(full_dataset, 'O3')\n",
    "# data = O3_test\n",
    "total = len(data)\n",
    "num_missing = data.isna().sum()\n",
    "plt.hist((num_missing / total)*100, bins=10, edgecolor = \"black\")\n",
    "plt.title(r'$CO$', fontsize=20)\n",
    "plt.xlabel('Proportion of missing data (%)', fontsize=18)\n",
    "plt.ylabel('No. of stations', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651ba1b",
   "metadata": {},
   "source": [
    "## Pre Graph Propagation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcedc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.orig = self.df.copy()\n",
    "        self.df['date'] = pd.to_datetime(self.df.date)\n",
    "        \n",
    "    def drop_null(self, nan_percent):\n",
    "        # drop column if proportion of NaN elements exceed the nan_percent\n",
    "        min_count = int(((100-nan_percent)/100)*self.df.shape[0] + 1)\n",
    "        return self.df.dropna(axis=1, thresh=min_count) \n",
    "        \n",
    "    def fill_mean(self):\n",
    "        return self.df.fillna(self.df.mean())\n",
    "    \n",
    "    def group(self, freq):\n",
    "        # group the data by the specified freq (month/year) and average across this period, then fill NaN values \n",
    "        df = self.df.groupby(pd.Grouper(key=\"date\", freq=freq)).mean()\n",
    "        return df\n",
    "    \n",
    "    def group_and_fill(self, freq):\n",
    "        # group the data by the specified freq (month/year) and average across this period, then fill NaN values \n",
    "        df = self.df.groupby(pd.Grouper(key=\"date\", freq=freq)).mean()\n",
    "        return df.ffill().bfill()\n",
    "    \n",
    "    def fill(self):\n",
    "        df = self.df.copy()\n",
    "        for col in df.columns.drop('date'):\n",
    "            df[col] = df[col].fillna(df.groupby([df.date.dt.year, df.date.dt.month])[col].transform('mean'))\n",
    "        return df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeAM():\n",
    "    def __init__(self, df):\n",
    "        am_shape = (df.shape[1], df.shape[1])\n",
    "        self.am = pd.DataFrame(np.zeros(shape=am_shape), columns=df.columns, index=df.columns)\n",
    "    \n",
    "    def euclidean_dist(self, df):\n",
    "        # np.linalg.norm(complete['TD0'].values - complete['BG3'].values) #test euclidean distance between two columns\n",
    "        dist_arr = squareform(pdist(df.transpose()))\n",
    "        return pd.DataFrame(dist_arr, columns=df.columns.unique(), index=df.columns.unique())\n",
    "    \n",
    "    def cosine_dist(self, df):\n",
    "        dist_arr = cosine_similarity(df.transpose())\n",
    "        np.fill_diagonal(dist_arr, 0)\n",
    "        return pd.DataFrame(dist_arr, columns=df.columns.unique(), index=df.columns.unique())\n",
    "    \n",
    "    def threshold_euclidean(self, df, threshold):\n",
    "        for col in df.columns:\n",
    "#             df.loc[df[col] > threshold, col] = 0\n",
    "#             df.loc[df[col] < threshold, col] = 1\n",
    "            df[col] = np.where(df[col]>=threshold, 1, 0)\n",
    "        np.fill_diagonal(df.values, 0)\n",
    "        return df\n",
    "    \n",
    "    def diagonal_degree(self, df):\n",
    "        diag_series = np.diag(df.sum())\n",
    "        degree_mat = pd.DataFrame(diag_series, columns=df.columns.unique(), index=df.columns.unique())\n",
    "        return degree_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44760611",
   "metadata": {},
   "source": [
    "## PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e05fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM10_test = full_dataset[full_dataset['species'] == 'PM10'].drop(columns=['species']).set_index('date').dropna(axis=1, how='all')\n",
    "PM10_data = select_species(full_dataset, 'PM10').dropna(axis=1, how='all')\n",
    "test_set, max_cols = get_test_set(PM10_data, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78476959",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM10_data.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_entries, initial, testing = force_gaps(test_set, proportion=0.25, seed=1)\n",
    "filled_data, euclidean = fill_and_refactor(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2276e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimise alpha\n",
    "res_alpha = minimize(compute_error, 0.3, args=(1.0, 2, initial, nan_entries, filled_data, euclidean))\n",
    "print(res_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890dad6d",
   "metadata": {},
   "source": [
    "### Error Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93905c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(threshold_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(threshold_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        # TEST VALUE\n",
    "        t_hop = 3\n",
    "        loss[i][j] = compute_error(val1, val2, t_hop, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(threshold_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X[:50], Y[:50], loss[:50], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and threshold')\n",
    "ax.set_xlabel('threshold')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Threshold: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ca968",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = compute_error(0.2061, 0.5, 2, initial, nan_entries, filled_data, euclidean)\n",
    "print(err / 2025**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss against alpha and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(L_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_threshold = 0.3698\n",
    "        loss[i][j] = compute_error(val1, t_threshold, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = 50\n",
    "lim2 = 4\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94167c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss against threshold and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.2, 2.0, 160)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(threshold_range), len(L_range)))\n",
    "for i, val1 in enumerate(threshold_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_alpha = 0.1163\n",
    "        loss[i][j] = compute_error(t_alpha, val1, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, threshold_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = len(threshold_range)\n",
    "lim2 = len(L_range)\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.invert_xaxis()\n",
    "ax.set_title('Error against threshold and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('threshold')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Threshold: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70085d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc09859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = 0.2061, 0.37, 2\n",
    "rmse_err = compute_error(0.2061, 0.37, 2, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "smape_err = compute_error(0.2061, 0.37, 2, initial, nan_entries, filled_data, euclidean, error_type='smape')\n",
    "print(f'RMSE Error: {rmse_err}')\n",
    "print(f'SMAPE Error: {smape_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = 0.116, 0.37, 3\n",
    "rmse_err = compute_error(0.116, 0.37, 3, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "smape_err = compute_error(0.116, 0.37, 3, initial, nan_entries, filled_data, euclidean, error_type='smape')\n",
    "print(f'RMSE Error: {rmse_err}')\n",
    "print(f'SMAPE Error: {smape_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56777b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise alpha\n",
    "\n",
    "t_alpha = 0.116\n",
    "t_threshold = 0.37\n",
    "t_hop = 3\n",
    "\n",
    "plt.figure(1)\n",
    "alpha_err = []\n",
    "alpha_range = np.linspace(0.0, 0.6, 101)\n",
    "for alpha in alpha_range:\n",
    "    err = compute_error(alpha, t_threshold, t_hop, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "    alpha_err.append(err)\n",
    "plt.plot(alpha_range, alpha_err)\n",
    "plt.title('RMSE Error', fontsize=18)\n",
    "plt.xlabel('Alpha', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "\n",
    "plt.figure(2)\n",
    "hop_err = []\n",
    "hop_range = np.arange(1, 6)\n",
    "for L in hop_range:\n",
    "    err = compute_error(t_alpha, t_threshold, L, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "    hop_err.append(err)\n",
    "plt.plot(hop_range, hop_err)\n",
    "plt.title('RMSE Error', fontsize=18)\n",
    "plt.xlabel('Hops', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "threshold_err = []\n",
    "threshold_range = np.linspace(0.1, 2.0, 101)\n",
    "for threshold in threshold_range:\n",
    "    err = compute_error(t_alpha, threshold, t_hop, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "    threshold_err.append(err)\n",
    "plt.plot(threshold_range, threshold_err)\n",
    "plt.title('RMSE Error', fontsize=18)\n",
    "plt.xlabel('Threshold', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "\n",
    "alpha_err = np.nan_to_num(alpha_err, nan=np.inf)\n",
    "print('Alpha error: ', min(alpha_err), alpha_range[np.argmin(alpha_err)])\n",
    "print('Hops error: ', min(hop_err), hop_range[np.argmin(hop_err)])\n",
    "print('Threshold error: ', min(threshold_err), threshold_range[np.argmin(threshold_err)])\n",
    "print(min(alpha_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad612e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMISED PARAMETERS\n",
    "pm10_alpha = 0.116\n",
    "pm10_threshold = 0.37\n",
    "pm10_hops = 3\n",
    "# error -> 9.810\n",
    "\n",
    "Z, A = compute_progation_matrix(filled_data, euclidean, threshold=pm10_threshold, L=pm10_hops, alpha=pm10_alpha)\n",
    "final = []\n",
    "for entry in nan_entries:\n",
    "    final.append(Z[entry])\n",
    "\n",
    "x = np.arange(200)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(initial, final)\n",
    "plt.plot(x, x, color='black')\n",
    "plt.title(r'Algorithm evaluation (RMSE = 9.81)')\n",
    "plt.xlabel(r'True PM$_{10}$ concentration ($\\mu g/mm^3$)')\n",
    "plt.ylabel(r'Propagated PM$_{10}$ concentration ($\\mu g/mm^3$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212fab1",
   "metadata": {},
   "source": [
    "### Scaled to full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a70f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, similarity = fill_and_refactor(PM10_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, A = compute_progation_matrix(full_data, similarity, threshold=pm10_threshold, L=pm10_hops, alpha=pm10_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fa7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = np.copy(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, column) in enumerate(PM10_data):\n",
    "    for (j, entry) in enumerate(np.asarray(PM10_data[column])): \n",
    "        if not np.isnan(entry):\n",
    "            corrected[j][i] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get similarity matrix from propagated data\n",
    "\n",
    "corrected_df = pd.DataFrame(corrected, columns=PM10_data.columns.unique(), index=PM10_data.index.unique())\n",
    "fd1, similarity = fill_and_refactor(corrected_df)\n",
    "propagated_df = pd.DataFrame(corrected, columns = PM10_data.columns.unique(), index = PM10_data.index.unique())\n",
    "propagated_df.to_csv('complete_PM10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350dc6bd",
   "metadata": {},
   "source": [
    "### PM10 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4691a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LAQN site codes\n",
    "region = 'London'\n",
    "url_sites = f\"http://api.erg.kcl.ac.uk/AirQuality/Information/MonitoringSites/GroupName={region}/Json\"\n",
    "               \n",
    "london_sites = requests.get(url_sites)\n",
    "sites_df = pd.DataFrame(london_sites.json()['Sites']['Site'])\n",
    "site_codes = sites_df[\"@SiteCode\"].tolist()\n",
    "print(len(site_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sites for each local authority\n",
    "site_map = {} # map between local authority codes and list of sites belonging to that local authority\n",
    "location_map = {} # map between local authority codes and local authority names\n",
    "# local_codes = set(sites_df['@LocalAuthorityCode'].unique()) # 1 - 33\n",
    "for i in range(1, 34):\n",
    "    code = str(i)\n",
    "    location_map[code] = sites_df[sites_df['@LocalAuthorityCode'] == code]['@LocalAuthorityName'].unique()[0]\n",
    "    res = sites_df[sites_df['@LocalAuthorityCode'] == code]['@SiteCode']\n",
    "    site_map[code] = []\n",
    "    for j, site in res.items():\n",
    "        site_map[code].append(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01dc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = PM10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eeee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dataframe2(df, freq='M'):\n",
    "    grouped_df = df.copy() \n",
    "    grouped_df = grouped_df.reset_index(level=0)\n",
    "    grouped_df['date'] = pd.to_datetime(grouped_df.date)\n",
    "    grouped_df = grouped_df.groupby(pd.Grouper(key=\"date\", freq=freq)).mean()\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d357e",
   "metadata": {},
   "source": [
    "## Time Series Plots (grouped by day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = propagated_df.index.values\n",
    "stations = {'LH0', 'GR5', 'BG2', 'KT4'}\n",
    "while len(stations) < 10:\n",
    "    sample = np.random.choice(grouped.columns.values[1:], 1)[0]\n",
    "    stations.add(sample)\n",
    "    \n",
    "for index, station in enumerate(stations):\n",
    "    plt.figure(index, figsize=(12, 4))\n",
    "    plt.plot(dates, propagated_df[station].values, color='black', linestyle='dotted')\n",
    "    plt.plot(dates, grouped[station].values, color='black')\n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b0d6c",
   "metadata": {},
   "source": [
    "## Time Series Plots (grouped by week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a900311",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_W = group_dataframe2(grouped, 'W')\n",
    "propagated_df_W = group_dataframe2(propagated_df, 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fed07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = propagated_df_W.index.values\n",
    "stations = {'LH0', 'GR5', 'BG2', 'KT4'}\n",
    "while len(stations) < 10:\n",
    "    sample = np.random.choice(grouped.columns.values[1:], 1)[0]\n",
    "    stations.add(sample)\n",
    "    \n",
    "for index, station in enumerate(stations):\n",
    "    plt.figure(index, figsize=(12, 4))\n",
    "    plt.plot(dates, propagated_df_W[station].values, color='black', linestyle='dotted')\n",
    "    plt.plot(dates, grouped_W[station].values, color='black')\n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88847e1",
   "metadata": {},
   "source": [
    "## Time Series Plots (grouped by month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_M = group_dataframe2(grouped, 'M')\n",
    "propagated_df_M = group_dataframe2(propagated_df, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = propagated_df_M.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b42fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(1996, 2021)\n",
    "months = np.arange(1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_count = missing_data_count(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8566eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = propagated_df_M.index.values\n",
    "stations = {'LH0', 'GR5', 'BG2', 'KT4'}\n",
    "while len(stations) < 10:\n",
    "    sample = np.random.choice(grouped.columns.values[1:], 1)[0]\n",
    "    stations.add(sample)\n",
    "    \n",
    "for index, station in enumerate(stations):\n",
    "    plt.figure(2*index, figsize=(12, 4))\n",
    "    plt.plot(dates, propagated_df_M[station].values, color='black', linestyle='dotted')\n",
    "    plt.plot(dates, grouped_M[station].values, color='black')\n",
    "    \n",
    "    missing_dates = get_missing_dates(grouped, station)\n",
    "    plt.scatter(missing_dates, propagated_df_M[station][missing_dates].values, marker='o', color='r', s = 3.0)\n",
    "    \n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    \n",
    "    plt.figure(2*index+1, figsize=(12, 4))\n",
    "    plt.scatter(dates, propagated_df_M[station].values, c=miss_count[station].values, marker='o', s=5.0, cmap='viridis')\n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce9638",
   "metadata": {},
   "source": [
    "### Borough Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfafa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = [x['color'] for x in plt.rcParams['axes.prop_cycle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = propagated_df_M.index.values\n",
    "for i in range(1, 34):\n",
    "    code = str(i)\n",
    "    cols = [site for site in site_map[code] if site in propagated_df_M.columns]\n",
    "#     for col in cols\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for j, col in enumerate(cols):\n",
    "        color = prop_cycle[j % len(prop_cycle)]\n",
    "        plt.plot(dates, grouped_M[col].values, color=color, label=f'{col}', linewidth=1)\n",
    "        plt.plot(dates, propagated_df_M[col].values, color=color, linestyle='dashed', linewidth=1)\n",
    "    plt.title(f'{location_map[code]}', fontsize=13)\n",
    "    plt.ylabel(\"PM$_{10}$ Concentrations (µg/m$^3$)\", fontsize=11)\n",
    "    plt.xlabel(\"Date\", fontsize=11)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e203b",
   "metadata": {},
   "source": [
    "### Similarity Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_stations = {}\n",
    "for station in A.columns:\n",
    "    similar_stations[station] = similarity[station].sort_values(ascending=False)[:5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e270c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_cycle = prop_cycle = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n",
    "plotted_stations = set()\n",
    "\n",
    "i = 1\n",
    "for station, similars in similar_stations.items():\n",
    "    plotted_stations.add(station)\n",
    "    \n",
    "    plt.figure(i, figsize=(12, 4))\n",
    "    plt.plot(dates, grouped_M[station].values, color=colour_cycle[0], label=f'{station}', linewidth=1)\n",
    "    plt.plot(dates, propagated_df_M[station].values, color=colour_cycle[0], linestyle='dashed', linewidth=1)\n",
    "    for j, similar in enumerate(similars):\n",
    "        plt.plot(dates, grouped_M[similar].values, color=colour_cycle[j+1], label=f'{similar}', linewidth=1)\n",
    "        plt.plot(dates, propagated_df_M[similar].values, color=colour_cycle[j+1], linestyle='dashed', linewidth=1)\n",
    "#         plt.plot(dates, propagated_df_M[similar].values, label=similar, color=colour_cycle[j+1])\n",
    "    plt.title(f\"Stations similar to: '{station}'\")\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(i+1)\n",
    "    similarity_list = similarity[station]\n",
    "    london_sites_gdf_sim = london_sites_gdf.copy()\n",
    "    london_sites_gdf_sim['Similarity'] = np.nan\n",
    "    for index, sim_val in similarity_list.items():\n",
    "        london_sites_gdf_sim.loc[london_sites_gdf_sim['@SiteCode'] == index, 'Similarity'] = sim_val\n",
    "    london_sites_gdf_sim = london_sites_gdf_sim[~london_sites_gdf_sim['Similarity'].isna()]\n",
    "\n",
    "    plot_on_map(london_sites_gdf_sim, london_gdf, data_column='Similarity', colorbar=True,\n",
    "                title=f\"Similarity map: '{station}'\", \n",
    "                data_markersize=5, fontsize=15,\n",
    "                map_edge_color=\"gray\", figsize=(15,7), axis=\"on\", mark=station)\n",
    "    \n",
    "    \n",
    "    i += 1\n",
    "    if i == 40:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec75b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN metadata\n",
    "london_landuse = gpd.read_file(path.join(folder, \"gis_osm_landuse_a_free_1.shp\"))\n",
    "print(london_landuse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_palette = {\n",
    "    'allotments': '#002fff',\n",
    "    'cemetery': 'gray',\n",
    "    'commercial': 'orange',\n",
    "    'farmland': '#002fff',\n",
    "    'farmyard': '#002fff',\n",
    "    'forest': 'green',\n",
    "    'grass': 'green',\n",
    "    'heath': 'green',\n",
    "    'industrial': '#4B0092',\n",
    "    'meadow': 'green',\n",
    "    'military': '#4B0092',\n",
    "    'nature_reserve': 'green',\n",
    "    'orchard': 'pink',\n",
    "    'park': 'green',\n",
    "    'quarry': 'gray',\n",
    "    'recreation_ground': 'green',\n",
    "    'residential': '#E3E3E3',\n",
    "    'retail': 'orange',\n",
    "    'scrub': 'green',\n",
    "}\n",
    "cmap = matplotlib.colors.ListedColormap([color for key, color in land_palette.items()])\n",
    "\n",
    "london_landuse.plot(figsize=(10,10), column='fclass', legend=True, cmap=cmap, legend_kwds={'loc': 'center right', 'bbox_to_anchor':(1.3,0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7944bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_osm_map(data_geodataframe, map_geodataframe, cmap, figsize=(20,10), colorbar=False, data_column='Similarity', title='LAQN Monitoring Station Distribution', mark=None, similars=None):\n",
    "    \n",
    "    base = data_geodataframe.plot(ax=map_geodataframe.plot(figsize=figsize, \n",
    "                                           column='fclass',\n",
    "                                           legend=False,\n",
    "                                           cmap=cmap,\n",
    "                                           alpha=0.5,\n",
    "                                           legend_kwds={'loc': 'center right', 'bbox_to_anchor':(1.3,0.5)}),\n",
    "                    color='black', marker='x', markersize=75, linewidths=3)\n",
    "    \n",
    "    if colorbar:\n",
    "        colorbar_max = data_geodataframe[data_column].max()\n",
    "        norm = plt.Normalize(data_geodataframe[data_column].min(), colorbar_max)\n",
    "        plt.colorbar(plt.cm.ScalarMappable(cmap=None, \n",
    "        norm=norm)).set_label(data_column)\n",
    "        \n",
    "    if mark:\n",
    "        marked = data_geodataframe[data_geodataframe['@SiteCode'] == mark]\n",
    "        marked.plot(ax=base, marker='o', color='black', markersize=100);\n",
    "\n",
    "    if mark and similar:\n",
    "        title = f'{title}\\n Similar stations: {similars}'\n",
    "    \n",
    "    plt.suptitle(title, fontsize=20)\n",
    "    plt.xlabel('Longitude', fontsize=14)\n",
    "    plt.ylabel('Latitude', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.axis(\"on\")\n",
    "    plt.savefig(f'images_PM10/{mark}_similarity.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f1ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get similarity maps\n",
    "\n",
    "# for station, similars in similar_stations.items():\n",
    "#     similarity_list = similarity[station]\n",
    "#     london_sites_gdf_sim = london_sites_gdf.copy()\n",
    "#     london_sites_gdf_sim['Similarity'] = np.nan\n",
    "#     for index, sim_val in similarity_list.items():  \n",
    "#         #ensure current station is most similar\n",
    "#         if index == station:\n",
    "#             london_sites_gdf_sim.loc[london_sites_gdf_sim['@SiteCode'] == index, 'Similarity'] = 100\n",
    "#         else:\n",
    "#             london_sites_gdf_sim.loc[london_sites_gdf_sim['@SiteCode'] == index, 'Similarity'] = sim_val\n",
    "        \n",
    "#     london_sites_gdf_sim = london_sites_gdf_sim[~london_sites_gdf_sim['Similarity'].isna()]\n",
    "    \n",
    "#     # MAP N MOST SIMILAR STATIONS\n",
    "# #     data_count=10\n",
    "# #     london_sites_gdf_sim = london_sites_gdf_sim.sort_values(by='Similarity', ascending=False)[:data_count]\n",
    "    \n",
    "#     # ... OR MAP STATIONS > 0.9*MAX\n",
    "#     max_similarity = london_sites_gdf_sim.sort_values(by='Similarity', ascending=False).iloc[1]['Similarity']\n",
    "#     london_sites_gdf_sim = london_sites_gdf_sim.loc[(london_sites_gdf_sim['Similarity'] >= 0.9*max_similarity)]\n",
    "      \n",
    "#     similars = london_sites_gdf_sim['@SiteCode'].values\n",
    "#     similars = np.setdiff1d(similars, station)\n",
    "#     plot_on_osm_map(london_sites_gdf_sim[:11], london_landuse, cmap, mark=station, title=f'LAQN $PM_{{10}}$ Dataset - Station {station}', similars=similars[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458691d",
   "metadata": {},
   "source": [
    "* Stations more dispersed compared to NO2 map\n",
    "* Compute average distance between similar stations to get geographical comparative measure (by borough, etc?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b78c7",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# Cross - PM10, NO2 (Global Mean)\n",
    "# -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e30ecc",
   "metadata": {},
   "source": [
    "## Dataset Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO2 dataset (df1 complete or incomplete)\n",
    "\n",
    "# complete_NO2 = pd.read_csv('complete_NO2.csv')\n",
    "# complete_NO2 = complete_NO2.rename(columns={c: c+'_NO2' for c in complete_NO2.columns if c not in ['date']})\n",
    "# df1 = complete_NO2\n",
    "\n",
    "NO2_data = select_species(full_dataset, 'NO2').dropna(axis=1, how='all')\n",
    "df1 = NO2_data.reset_index()\n",
    "df1 = df1.rename(columns={c: c+'_NO2' for c in df1.columns if c not in ['date']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd379b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM10 dataset (df2 complete or incomplete)\n",
    "\n",
    "# complete_PM10 = propagated_df.add_suffix('_PM10')\n",
    "# df2 = complete_PM10.reset_index()\n",
    "# df2 = df2.drop(['date'], axis=1)\n",
    "\n",
    "PM10_data = select_species(full_dataset, 'PM10').dropna(axis=1, how='all')\n",
    "df2 = PM10_data.reset_index()\n",
    "df2 = df2.drop(['date'], axis=1)\n",
    "df2 = df2.add_suffix('_PM10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b989dc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_data = pd.concat([df1, df2], axis=1)\n",
    "cross_data = cross_data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5ee0a",
   "metadata": {},
   "source": [
    "## Controlled Cross Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, max_cols = get_test_set(cross_data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87222b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.columns[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(test_set.size)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[np.where(test % test_set.shape[1] > 27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_gaps(test_set, proportion=0.25, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    testing = test_set.copy()\n",
    "    \n",
    "    num_gaps = int(proportion * test_set.size)\n",
    "\n",
    "    # Replace random entries with NaNs\n",
    "    num_entries = test_set.size\n",
    "    entry_list = np.arange(num_entries)\n",
    "    entry_list = entry_list[np.where(entry_list % test_set.shape[1] > 27)] # gaps only in PM10 data (27 boundary)\n",
    "    nan_indices = np.random.choice(entry_list, num_gaps, replace=False)\n",
    "    nan_entries = [(num // test_set.shape[1], num % test_set.shape[1]) for num in nan_indices]\n",
    "\n",
    "    initial = []\n",
    "    for entry in nan_entries:\n",
    "        initial.append(testing.iloc[entry])\n",
    "        testing.iloc[entry] = np.nan\n",
    "    return nan_entries, initial, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262df4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global mean normalisation\n",
    "missing_prop = 0.10 # VARIABLE\n",
    "nan_entries, initial, testing = force_gaps(test_set, proportion=missing_prop, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ab4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data, euclidean = fill_and_refactor(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559b0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885736d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d21902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_graph_propagation(X, A, w, L, a=0.5, b=0.5):\n",
    "    D_list = np.sum(A, axis=1) # D matrix\n",
    "    w = np.array(w) \n",
    "    prop_matrix = np.diag(D_list**-a).dot(A).dot(np.diag(D_list**-b)) # DAD^(-1)\n",
    "    prop_matrix = np.nan_to_num(prop_matrix) # convert NaNs to 0s\n",
    "    \n",
    "    pi = np.zeros_like(X)\n",
    "    r = X\n",
    "    for i in range(L):\n",
    "        Y_i = w[i:].sum()\n",
    "        Y_iplus = w[i+1:].sum()\n",
    "        \n",
    "        # update pi estimate\n",
    "        q = (w[i]/Y_i) * r\n",
    "        pi += q\n",
    "        \n",
    "        # update r\n",
    "        r = (Y_i/Y_iplus) * prop_matrix.dot(r.T).T\n",
    "        \n",
    "    q = w[L]/w[L:].sum() * r\n",
    "    pi += q\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d783d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_error(initial, final):\n",
    "    return np.linalg.norm(np.array(initial) - np.array(final)) / len(initial)**0.5\n",
    "\n",
    "def smape_error(initial, final):\n",
    "    initial, final = np.array(initial), np.array(final)\n",
    "    num = np.absolute(initial - final)\n",
    "    den = (np.absolute(initial) + np.absolute(final)) / 2\n",
    "    elems = num/den\n",
    "    return np.sum(elems) / elems.size\n",
    "\n",
    "def compute_error(alpha, threshold, L, initial, nan_entries, data, euclideans, error_type='rmse'):\n",
    "    prop = GraphPropagation()\n",
    "    A = prop.threshold_am(euclideans, threshold)\n",
    "    A.iloc[28:, 28:] = 0\n",
    "    w = [alpha*(1-alpha)**i for i in range(10)]\n",
    "\n",
    "    # Apply algorithm\n",
    "    array_data = data.to_numpy()\n",
    "    Z = basic_graph_propagation(array_data, A, w, L)\n",
    "    \n",
    "    final = []\n",
    "    for entry in nan_entries:\n",
    "        final.append(Z[entry])\n",
    "    \n",
    "    if error_type == 'rmse':\n",
    "        error = rmse_error(initial, final)\n",
    "    elif error_type == 'smape':\n",
    "        error = smape_error(initial, final)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e1ede",
   "metadata": {},
   "source": [
    "### Error Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599f46f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c814bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.linspace(0.05, 0.3, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(threshold_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(threshold_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        # TEST VALUE\n",
    "        t_hop = 2\n",
    "        loss[i][j] = compute_error(val1, val2, t_hop, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(threshold_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X[:50], Y[:50], loss[:50], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and threshold')\n",
    "ax.set_xlabel('threshold')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Threshold: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df9773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = compute_error(0.22244897959183674, 1.3, 2, initial, nan_entries, filled_data, euclidean)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss against alpha and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(L_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_threshold = 0.709433962264151\n",
    "        loss[i][j] = compute_error(val1, t_threshold, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = 50\n",
    "lim2 = 4\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f46ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot loss against threshold and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.2, 2.0, 160)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(threshold_range), len(L_range)))\n",
    "for i, val1 in enumerate(threshold_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_alpha = 0.3776\n",
    "        loss[i][j] = compute_error(t_alpha, val1, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, threshold_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = len(threshold_range)\n",
    "lim2 = len(L_range)\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.invert_xaxis()\n",
    "ax.set_title('Error against threshold and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('threshold')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Threshold: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_alpha = 0.3776\n",
    "cross_threshold = 0.7094\n",
    "cross_hops = 1\n",
    "\n",
    "rmse_err = compute_error(cross_alpha, cross_threshold, cross_hops, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "smape_err = compute_error(cross_alpha, cross_threshold, cross_hops, initial, nan_entries, filled_data, euclidean, error_type='smape')\n",
    "print(f'RMSE Error: {rmse_err}')\n",
    "print(f'SMAPE Error: {smape_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMISED PARAMETERS\n",
    "\n",
    "Z, A = compute_progation_matrix(filled_data, euclidean, threshold=cross_threshold, L=cross_hops, alpha=cross_alpha)\n",
    "final = []\n",
    "for entry in nan_entries:\n",
    "    final.append(Z[entry])\n",
    "\n",
    "x = np.arange(100)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(initial, final)\n",
    "plt.plot(x, x, color='black')\n",
    "plt.title('Algorithm evaluation (RMSE = 9.002)')\n",
    "plt.xlabel('True pollutant concentration')\n",
    "plt.ylabel('Propagated pollutant concentration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b5f50",
   "metadata": {},
   "source": [
    "### Scaled to full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, similarity_cross = fill_and_refactor(cross_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L(matrix):\n",
    "    total = np.zeros_like(matrix)\n",
    "    \n",
    "    i = 0\n",
    "    while np.count_nonzero(total) != matrix.size:\n",
    "        i += 1\n",
    "        total += np.linalg.matrix_power(matrix, i)\n",
    "        if i == 10:\n",
    "            break\n",
    "    return i\n",
    "\n",
    "def compute_progation_matrix(data, euclideans, threshold, L=None, alpha=None, w=np.array([1, 0, 0, 0])):\n",
    "    prop = GraphPropagation()\n",
    "    A = prop.threshold_am(euclideans, threshold)\n",
    "    A.iloc[201:, 201:] = 0\n",
    "\n",
    "    if alpha:\n",
    "        w = [alpha*(1-alpha)**i for i in range(10)]\n",
    "    if not L:\n",
    "        L = get_L(A)\n",
    "\n",
    "    # Apply algorithm\n",
    "    array_data = data.to_numpy()\n",
    "    Z = basic_graph_propagation(array_data, A, w, L)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, A = compute_progation_matrix(full_data, similarity_cross, threshold=cross_threshold, L=cross_hops, alpha=cross_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = np.copy(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y = 0\n",
    "for (i, column) in enumerate(cross_data):\n",
    "    for (j, entry) in enumerate(np.asarray(cross_data[column])):\n",
    "        x += 1\n",
    "        if not np.isnan(entry):\n",
    "            y += 1\n",
    "            corrected[j][i] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get similarity matrix from propagated data\n",
    "\n",
    "corrected_df = pd.DataFrame(corrected, columns=cross_data.columns.unique(), index=cross_data.index.unique())\n",
    "fd1, similarity_cross = fill_and_refactor(corrected_df)\n",
    "cross_propagated_df = pd.DataFrame(corrected, columns = cross_data.columns.unique(), index = cross_data.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916041bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_propagated_df.to_csv('PM10_from_NO2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cross stations with the greatest similarity\n",
    "top_similarities = (-similarity_cross.stack()).argsort()[:500].values\n",
    "for ind, i in enumerate(top_similarities):\n",
    "    idx = similarity_cross.stack().index[i]\n",
    "    station1 = idx[0]\n",
    "    station2 = idx[1]\n",
    "    if (station1.endswith(\"NO2\") and station2.endswith(\"PM10\")):\n",
    "        print(f'{station1}, {station2}: {similarity_cross[station1][station2]}')\n",
    "#     print(f'{station1}, {station2}: {similarity_cross[station1][station2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cross.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc500eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "a = np.random.random((16, 16))\n",
    "# diag_similarity_cross = np.fill_diagonal(A.values, 3)\n",
    "plt.imshow(A, cmap='magma', interpolation='nearest', vmin=0, vmax=1)\n",
    "plt.xlabel(r'$NO_2$ and $PM_{10}$ stations')\n",
    "plt.ylabel(r'$NO_2$ and $PM_{10}$ stations')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM10_data.mean().mean()\n",
    "\n",
    "# Smaller mean value means that station vectors are generally closer to each other (hence bright patches for PM10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e066d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO2_data.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff109819",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cross.mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cross_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dataframe2(df, freq='M'):\n",
    "    grouped_df = df.copy() \n",
    "    grouped_df = grouped_df.reset_index(level=0)\n",
    "    grouped_df['date'] = pd.to_datetime(grouped_df.date)\n",
    "    grouped_df = grouped_df.groupby(pd.Grouper(key=\"date\", freq=freq)).mean()\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda415d",
   "metadata": {},
   "source": [
    "## Time Series Plots (grouped by month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "propagated_df = cross_propagated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aea79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_M = group_dataframe2(grouped, 'M')\n",
    "propagated_df_M = group_dataframe2(propagated_df, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = propagated_df_M.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(1996, 2021)\n",
    "months = np.arange(1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_count = missing_data_count(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ea60e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = propagated_df_M.index.values\n",
    "stations = {'ST3_NO2', 'HI1_NO2', 'SK1_PM10', 'ST3_NO2'}\n",
    "while len(stations) < 10:\n",
    "    sample = np.random.choice(grouped.columns.values[1:], 1)[0]\n",
    "    stations.add(sample)\n",
    "    \n",
    "for index, station in enumerate(stations):\n",
    "    plt.figure(2*index, figsize=(12, 4))\n",
    "    plt.plot(dates, propagated_df_M[station].values, color='black', linestyle='dotted')\n",
    "    plt.plot(dates, grouped_M[station].values, color='black')\n",
    "    \n",
    "    missing_dates = get_missing_dates(grouped, station)\n",
    "    plt.scatter(missing_dates, propagated_df_M[station][missing_dates].values, marker='o', color='r', s = 3.0)\n",
    "    \n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    \n",
    "    plt.figure(2*index+1, figsize=(12, 4))\n",
    "    plt.scatter(dates, propagated_df_M[station].values, c=miss_count[station].values, marker='o', s=5.0, cmap='viridis')\n",
    "    plt.title(f'Station: {station}')\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('PM$_{10}$ Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6845bd8",
   "metadata": {},
   "source": [
    "### Similarity Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aa6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = similarity_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_stations = {}\n",
    "for station in A.columns:\n",
    "    similar_stations[station] = similarity[station].sort_values(ascending=False)[:5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_cycle = prop_cycle = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n",
    "plotted_stations = set()\n",
    "\n",
    "i = 0\n",
    "for station, similars in similar_stations.items():\n",
    "    plotted_stations.add(station)\n",
    "    \n",
    "    plt.figure(i, figsize=(12, 4))\n",
    "    plt.plot(dates, grouped_M[station].values, color=colour_cycle[0], label=f'{station}', linewidth=1)\n",
    "    plt.plot(dates, propagated_df_M[station].values, color=colour_cycle[0], linestyle='dashed', linewidth=1)\n",
    "    for j, similar in enumerate(similars):\n",
    "        plt.plot(dates, grouped_M[similar].values, color=colour_cycle[j+1], label=f'{similar}', linewidth=1)\n",
    "        plt.plot(dates, propagated_df_M[similar].values, color=colour_cycle[j+1], linestyle='dashed', linewidth=1)\n",
    "#         plt.plot(dates, propagated_df_M[similar].values, label=similar, color=colour_cycle[j+1])\n",
    "    plt.title(f\"Stations similar to: '{station}'\")\n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('Pollutant Concentrations (µg/m$^3$)', fontsize=10)\n",
    "    plt.legend()\n",
    "    \n",
    "#     plt.figure(i+1)\n",
    "#     similarity_list = similarity[station]\n",
    "#     london_sites_gdf_sim = london_sites_gdf.copy()\n",
    "#     london_sites_gdf_sim['Similarity'] = np.nan\n",
    "#     for index, sim_val in similarity_list.items():\n",
    "#         london_sites_gdf_sim.loc[london_sites_gdf_sim['@SiteCode'] == index, 'Similarity'] = sim_val\n",
    "#     london_sites_gdf_sim = london_sites_gdf_sim[~london_sites_gdf_sim['Similarity'].isna()]\n",
    "\n",
    "#     plot_on_map(london_sites_gdf_sim, london_gdf, data_column='Similarity', colorbar=True,\n",
    "#                 title=f\"Similarity map: '{station}'\", \n",
    "#                 data_markersize=5, fontsize=15,\n",
    "#                 map_edge_color=\"gray\", figsize=(15,7), axis=\"on\", mark=station)\n",
    "    \n",
    "    i += 2\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN metadata\n",
    "london_landuse = gpd.read_file(path.join(folder, \"gis_osm_landuse_a_free_1.shp\"))\n",
    "print(london_landuse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9428f19",
   "metadata": {},
   "source": [
    "### Cross Similarity Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aeb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cross stations with the greatest similarity\n",
    "top_similarities = (-similarity_cross.stack()).argsort()[:500].values\n",
    "cross_stations = []\n",
    "for ind, i in enumerate(top_similarities):\n",
    "    idx = similarity_cross.stack().index[i]\n",
    "    station1 = idx[0]\n",
    "    station2 = idx[1]\n",
    "    if (station1.endswith(\"NO2\") and station2.endswith(\"PM10\")):\n",
    "        cross_stations.append(idx)\n",
    "        print(f'{station1}, {station2}: {similarity_cross[station1][station2]}')\n",
    "#     print(f'{station1}, {station2}: {similarity_cross[station1][station2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(cross_stations):\n",
    "    plt.figure(i, figsize=(12, 4))\n",
    "    station1 = pair[0]\n",
    "    station2 = pair[1]\n",
    "                    \n",
    "    plt.plot(dates, propagated_df_M[station1].values, color=colour_cycle[0], label=f'{station1}', linewidth=1)\n",
    "    plt.plot(dates, propagated_df_M[station2].values, color=colour_cycle[1], label=f'{station2}', linewidth=1)\n",
    "    \n",
    "    plt.xlabel('date', fontsize=10)\n",
    "    plt.ylabel('Pollutant concentrations (µg/m$^3$)', fontsize=10)\n",
    "    plt.legend()\n",
    "    \n",
    "#     plt.plot(dates, propagated_df_M[station1].values, color=colour_cycle[0], linestyle='dashed', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_M.mean()['CE2_NO2'])\n",
    "print(grouped_M.mean()['VS1_PM10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_M[station1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e9f42",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# Cross - PM10, NO2 (Grouped Mean)\n",
    "# -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_data = select_species(full_dataset, 'NO2').dropna(axis=1, how='all')\n",
    "# NO2_norm = NO2_data.divide(NO2_data.mean().mean())\n",
    "df1_t = NO2_norm.reset_index()\n",
    "df1_t = df1_t.rename(columns={c: c+'_NO2' for c in df1_t.columns if c not in ['date']})\n",
    "\n",
    "PM10_data = select_species(full_dataset, 'PM10').dropna(axis=1, how='all')\n",
    "# PM10_norm = PM10_data.divide(PM10_data.mean().mean())\n",
    "df2_t = PM10_norm.reset_index()\n",
    "df2_t = df2_t.drop(['date'], axis=1)\n",
    "df2_t = df2_t.add_suffix('_PM10')\n",
    "\n",
    "cross_data2 = pd.concat([df1_t, df2_t], axis=1)\n",
    "cross_data2 = cross_data2.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96002000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_and_refactor(gap_data):\n",
    "    filled_data = gap_data.ffill().bfill()\n",
    "    am = ComputeAM(filled_data)\n",
    "    euclidean_am = am.euclidean_dist(filled_data) # initially, the larger the value, the more distant and the less similar\n",
    "\n",
    "    mean = euclidean_am.mean().mean() \n",
    "    refactored = (mean / euclidean_am)  # Larger values represent more similar stations\n",
    "    np.fill_diagonal(refactored.values, 0)\n",
    "    return filled_data, refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, max_cols = get_test_set(cross_data2, 500)\n",
    "nan_entries, initial, testing = force_gaps(test_set, proportion=0.25, seed=5)\n",
    "filled_data, euclidean = fill_and_refactor(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e19ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0d2ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimise alpha\n",
    "res_alpha = minimize(compute_error, 0.3, args=(1.0, 2, initial, nan_entries, filled_data, euclidean))\n",
    "print(res_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8734a",
   "metadata": {},
   "source": [
    "### Error Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(threshold_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(threshold_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        # TEST VALUE\n",
    "        t_hop = 2\n",
    "        loss[i][j] = compute_error(val1, val2, t_hop, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(threshold_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X[:50], Y[:50], loss[:50], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and threshold')\n",
    "ax.set_xlabel('threshold')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Threshold: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = compute_error(0.22244897959183674, 1.3, 2, initial, nan_entries, filled_data, euclidean)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20368615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss against alpha and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.5, 2.0, 16)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(alpha_range), len(L_range)))\n",
    "for i, val1 in enumerate(alpha_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_threshold = 1.366\n",
    "        loss[i][j] = compute_error(val1, t_threshold, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, alpha_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = 50\n",
    "lim2 = 4\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Error against alpha and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Alpha: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b2c6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot loss against threshold and L (hops)\n",
    "\n",
    "alpha_range = np.linspace(0.1, 0.5, 50)\n",
    "threshold_range = np.linspace(0.2, 2.0, 160)\n",
    "L_range = np.arange(1, 6)\n",
    "\n",
    "loss = np.zeros((len(threshold_range), len(L_range)))\n",
    "for i, val1 in enumerate(threshold_range): \n",
    "    for j, val2 in enumerate(L_range):\n",
    "        val1 = round(val1, 2)\n",
    "        val2 = round(val2, 2)\n",
    "        \n",
    "        t_alpha = 0.2224\n",
    "        loss[i][j] = compute_error(t_alpha, val1, val2, initial, nan_entries, filled_data, euclidean)\n",
    "        \n",
    "X, Y = np.meshgrid(L_range, threshold_range)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "lim1 = len(threshold_range)\n",
    "lim2 = len(L_range)\n",
    "surf = ax.plot_surface(X[:lim1, :lim2], Y[:lim1, :lim2], loss[:lim1, :lim2], cmap='plasma', linewidth=2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.invert_xaxis()\n",
    "ax.set_title('Error against threshold and L (hops)')\n",
    "ax.set_xlabel('L (hops)')\n",
    "ax.set_ylabel('threshold')\n",
    "ax.set_zlabel('error')\n",
    "\n",
    "shape = np.unravel_index(loss.argmin(), loss.shape)\n",
    "print(f'Hops: {X[shape]}')\n",
    "print(f'Threshold: {Y[shape]}')\n",
    "print(f'Loss: {loss.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c67567",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_alpha = 0.22245\n",
    "cross_threshold = 1.274\n",
    "cross_hops = 2\n",
    "\n",
    "rmse_err = compute_error(cross_alpha, cross_threshold, cross_hops, initial, nan_entries, filled_data, euclidean, error_type='rmse')\n",
    "smape_err = compute_error(cross_alpha, cross_threshold, cross_hops, initial, nan_entries, filled_data, euclidean, error_type='smape')\n",
    "print(f'RMSE Error: {rmse_err}')\n",
    "print(f'SMAPE Error: {smape_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaaee9",
   "metadata": {},
   "source": [
    "# Cross Test Set v1 (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df2], axis=1)\n",
    "result = result.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dataset\n",
    "\n",
    "species_comp = ['NO2', 'PM10']\n",
    "common_cols = np.intersect1d(NO2_raw.columns, PM10_raw.columns).tolist()[:-1] # 147 common stations between NO2 and PM10 datasets\n",
    "\n",
    "# Select columns of data common to both datasets\n",
    "PM10_data = select_species(full_dataset, 'PM10')[common_cols]\n",
    "NO2_data = select_species(full_dataset, 'NO2')[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_cross_set(df, df2, num_valid_values=500):\n",
    "    max_size = 0\n",
    "    max_index = 0\n",
    "\n",
    "    for i in range(0, df.shape[0], 5):\n",
    "        test = df.iloc[i:].isnull()\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        res = test.eq(True).idxmax() # count of consecutive readings per station\n",
    "        size = res[res > num_valid_values].size # number of stations with over specified number of of readings\n",
    "        cols = res[res > num_valid_values].keys()\n",
    "\n",
    "        test = df2[cols].iloc[i:].isnull()\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        res = test.eq(True).idxmax() # count of consecutive readings per station\n",
    "        size = res[res > num_valid_values].size # number of stations with over specified number of of readings\n",
    "\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            max_index = i\n",
    "        \n",
    "    test = df.iloc[max_index:].isnull()\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    res = test.eq(True).idxmax()\n",
    "    max_cols = res[res > num_valid_values].keys()\n",
    "    test_set = df[max_cols].iloc[max_index:max_index+num_valid_values]\n",
    "\n",
    "    test = df2[max_cols].iloc[max_index:].isnull()\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    res = test.eq(True).idxmax()\n",
    "    max_cols = res[res > num_valid_values].keys()\n",
    "    test_set2 = df2[max_cols].iloc[max_index:max_index+num_valid_values]\n",
    "    \n",
    "    test_set = test_set[max_cols]\n",
    "    \n",
    "    return test_set, test_set2, max_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_control, PM10_control, max_cols = get_test_cross_set(NO2_data, PM10_data, num_valid_values=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98289972",
   "metadata": {},
   "source": [
    "## Cross Correlate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f8be9",
   "metadata": {},
   "source": [
    "* Normalise dataset by subtracting mean and dividing by standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_control.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b3ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO2_normalised = NO2_control.sub(NO2_control.mean(), axis='columns').div(NO2_control.std(), axis='columns')\n",
    "NO2_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f0838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PM10_normalised = PM10_control.sub(PM10_control.mean(), axis='columns').div(PM10_control.std(), axis='columns')\n",
    "PM10_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_AM(df1, df2):\n",
    "    size = df1.columns.size\n",
    "    column_list = df1.columns.unique()\n",
    "    cross_AM = pd.DataFrame(np.zeros((size,size)), columns=column_list, index=column_list)\n",
    "    for column in column_list: # fills rows for single column\n",
    "        base = df1[column]\n",
    "        for column2 in column_list: # fills all columns\n",
    "            compare = df2[column2]\n",
    "            dist = np.linalg.norm(base - compare)\n",
    "            cross_AM[column][column2] = dist # cross_AM[select column][select row]\n",
    "    \n",
    "    return cross_AM\n",
    "\n",
    "compute_cross_AM(NO2_normalised, PM10_normalised)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
